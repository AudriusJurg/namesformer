{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1b3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257fe168",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46a6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_dataset = pd.read_csv('data/pop_vardai_vyrai.txt',delimiter=' ',encoding=\"UTF-16\")\n",
    "female_dataset = pd.read_csv('data/pop_vardai_moterys.txt',delimiter=' ',encoding=\"UTF-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a01ee6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            name  popularity\n",
      "0           Abas           0\n",
      "1        Abdijus           0\n",
      "2        Abdonas           0\n",
      "3         Abdula           0\n",
      "4         Abelis           2\n",
      "...          ...         ...\n",
      "3845  Žigimantas          25\n",
      "3846  Žigymantas           0\n",
      "3847    Žigintas           0\n",
      "3848    Žilvynas           0\n",
      "3849    Žimantas           0\n",
      "\n",
      "[3850 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(male_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc2595c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4235, 2)\n",
      "(3850, 2)\n"
     ]
    }
   ],
   "source": [
    "print(female_dataset.shape)\n",
    "print(male_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793060fd",
   "metadata": {},
   "source": [
    "We have around 8085 names total. How many of those are 0? How many are >100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3712063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of male names with 0 popularity: 1391\n",
      "Number of male names with >=100 popularity: 404\n",
      "Max popularity for male name: 17236\n",
      "\n",
      "Average popularity for male name: 141.73\n",
      "\n",
      "Number of female names with 0 popularity: 1583\n",
      "Number of female names with >=100 popularity: 499\n",
      "Average popularity for female name: 164.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of male names with 0 popularity: {(male_dataset['popularity']==0).sum()}\")\n",
    "print(f\"Number of male names with >=100 popularity: {(male_dataset['popularity']>=100).sum()}\")\n",
    "print(f\"Max popularity for male name: {(male_dataset['popularity']).max()}\")\n",
    "print(f\"Average popularity for male name: {(male_dataset['popularity']).mean().round(2)}\\n\")\n",
    "print(f\"Number of female names with 0 popularity: {(female_dataset['popularity']==0).sum()}\")\n",
    "print(f\"Number of female names with >=100 popularity: {(female_dataset['popularity']>=100).sum()}\")\n",
    "print(f\"Max popularity for female name: {(female_dataset['popularity']).max()}\")\n",
    "print(f\"Average popularity for female name: {(female_dataset['popularity']).mean().round(2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd5fb7",
   "metadata": {},
   "source": [
    "There are a lot of words with 0 popularity in both datasets. These names are either improper, foreign or otherwise unused in the modern naming scheme.\n",
    "\n",
    "For the sake of a more accurate model, we will be omiting names with 0 popularity.\n",
    "\n",
    "Additionally, we will try to see what kind of model we can get if we take the n most popular names as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7f35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fede1162",
   "metadata": {},
   "source": [
    "# Namesformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f709fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.names = pd.read_csv(csv_file)['name'].values\n",
    "        self.chars = sorted(list(set(''.join(self.names) + ' ')))  # Including a padding character\n",
    "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.int_to_char = {i: c for c, i in self.char_to_int.items()}\n",
    "        self.vocab_size = len(self.chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx] + ' '  # Adding padding character at the end\n",
    "        encoded_name = [self.char_to_int[char] for char in name]\n",
    "        return torch.tensor(encoded_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ef057c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'm', 'm', 'a', ' ']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = NameDataset('data/english_names.txt')\n",
    "[dataset.int_to_char[num] for num in dataset[0].numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c907aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    padded_seqs = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    input_seq = padded_seqs[:, :-1]\n",
    "    target_seq = padded_seqs[:, 1:]\n",
    "    return input_seq, target_seq\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f1c9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, forward_expansion):\n",
    "        super(MinimalTransformer, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1)).unsqueeze(0)\n",
    "        x = self.embed(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc96d0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d438415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2097248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Ensure the model is in training mode\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output.transpose(1, 2), target_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        average_loss = total_loss / batch_count\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98bd6f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jusau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 1.4406895644650488\n",
      "Epoch 2, Average Loss: 1.4062788443651029\n",
      "Epoch 3, Average Loss: 1.398684309687681\n",
      "Epoch 4, Average Loss: 1.401524607173935\n",
      "Epoch 5, Average Loss: 1.3975820101187852\n",
      "Epoch 6, Average Loss: 1.3991666962643583\n",
      "Epoch 7, Average Loss: 1.3947380464709924\n",
      "Epoch 8, Average Loss: 1.3956587194444652\n",
      "Epoch 9, Average Loss: 1.3965631889963817\n",
      "Epoch 10, Average Loss: 1.3933474934267664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MinimalTransformer(vocab_size=dataset.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n",
    "train_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce9936",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e5bfc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, dataset, start_str='a', max_length=20, temperature=1.0):\n",
    "    assert temperature > 0, \"Temperature must be greater than 0\"\n",
    "    model.eval()  # Switch model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Convert start string to tensor\n",
    "        chars = [dataset.char_to_int[c] for c in start_str]\n",
    "        input_seq = torch.tensor(chars).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        output_name = start_str\n",
    "        for _ in range(max_length - len(start_str)):\n",
    "            output = model(input_seq)\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            logits = output[0, -1] / temperature\n",
    "            probabilities = torch.softmax(logits, dim=0)\n",
    "            \n",
    "            # Sample a character from the probability distribution\n",
    "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
    "            next_char = dataset.int_to_char[next_char_idx]\n",
    "            \n",
    "            if next_char == ' ':  # Assume ' ' is your end-of-sequence character\n",
    "                break\n",
    "            \n",
    "            output_name += next_char\n",
    "            # Update the input sequence for the next iteration\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]])], dim=1)\n",
    "        \n",
    "        return output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cda0cea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More confident:\n",
      "  raia\n",
      "  rasly\n",
      "  raris\n",
      "  rion\n",
      "  reniely\n",
      "  rari\n",
      "  riaynin\n",
      "  rene\n",
      "  relann\n",
      "  reva\n",
      "\n",
      "More diverse/creative:\n",
      "  rucadom\n",
      "  rijnan\n",
      "  ro\n",
      "  rampchia\n",
      "  rihey\n",
      "  royciy\n",
      "  ronlyawn\n",
      "  rnsud\n",
      "  rihiepph\n",
      "  rahr\n"
     ]
    }
   ],
   "source": [
    "print('More confident:')\n",
    "for _ in range(10):\n",
    "    print(' ', sample(model, dataset, start_str='r', temperature=0.5))  # More confident\n",
    "\n",
    "print('\\nMore diverse/creative:')\n",
    "for _ in range(10):\n",
    "    print(' ', sample(model, dataset, start_str='r', temperature=1.5))  # More diverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c80fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/namesformer_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
